# Docker Compose Configuration for cc-vec-bot
# Copy this file to .env and customize values as needed

# Run: docker compose up --build

# Inference model to use (tinyllama, llama2, llama3.2:3b, etc.)
# recommended: tinyllama for local testing (700MB), llama3.2:3B for production
INFERENCE_MODEL=tinyllama

# LLM model files are large.
# Set to 1 to pre-fetch the model at build time (increases image size and build time)
# Set to 0 to fetch when the image is run (smaller, faster build but redundant runtime fetch of large models)
PREFETCH_MODEL=1

# Ports
LLAMA_STACK_PORT=5001
CHATBOT_PORT=8008

# Streaming mode for chatbot responses (0=off, 1=on)
OLLAMA_STREAMING=1

# Ollama URL (optional - defaults to local Ollama on port 11434)
# Leave empty to use built-in Ollama, or set to external instance
# Examples:
#   OLLAMA_URL=http://host.docker.internal:11434
#   OLLAMA_URL=http://192.168.1.100:11435
# OLLAMA_URL=

# ChromaDB URL (optional - for persistent vector storage)
# CHROMADB_URL=http://localhost:8000

