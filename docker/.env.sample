# Docker Compose Configuration for cc-vec-bot
# Copy this file to .env and customize values as needed

# Run: docker compose up --build

# Inference model to use (tinyllama, llama2, llama3.2:3b, etc.)
# recommended: tinyllama for local testing (700MB), llama3.2:3B for production
INFERENCE_MODEL=tinyllama

# LLM model files are large.
# Set to 1 to pre-fetch the model at build time (increases image size and build time)
# Set to 0 to fetch when the image is run (smaller, faster build but redundant runtime fetch of large models)
PREFETCH_MODEL=1

# Ports
LLAMA_STACK_PORT=5001
CHATBOT_PORT=8008
OLLAMA_PORT=11434

# External ollama/chromadb URL
# Set these if you want to customize llama-stack's behavior
# OLLAMA_URL=http://localhost:11434
# CHROMADB_URL=http://localhost:8000

