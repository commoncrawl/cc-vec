FROM llamastack/distribution-starter:0.4.1
LABEL maintainer="damian@commoncrawl.org"

USER root

# Install minimal dependencies required by the Ollama install script
RUN apt-get update \
    && apt-get install -y --no-install-recommends curl ca-certificates gnupg zstd \
    && rm -rf /var/lib/apt/lists/*

# Install Ollama (for local inference when OLLAMA_URL is not set)
RUN curl -fsSL https://ollama.ai/install.sh | sh

ENV PATH="/usr/local/bin:${PATH}"

# ---------------------------------------------------------------------------
# Build-time model pre-fetch (optional)
# Set PREFETCH_MODEL=1 to bake the model into the image at build time.
# This makes the image larger but faster to start.
#
# Build examples:
#   docker build --build-arg PREFETCH_MODEL=1 -t cc-vec-bot .           # bake tinyllama
#   docker build --build-arg PREFETCH_MODEL=1 --build-arg INFERENCE_MODEL=llama3.2:3b -t cc-vec-bot .
#   docker build -t cc-vec-bot .                                         # no prefetch (default)
# ---------------------------------------------------------------------------
ARG PREFETCH_MODEL=0
ARG INFERENCE_MODEL=tinyllama

# Pre-fetch model at build time if PREFETCH_MODEL=1
# Requires starting ollama serve temporarily during build
RUN if [ "$PREFETCH_MODEL" = "1" ]; then \
        echo "Pre-fetching model: ${INFERENCE_MODEL}"; \
        ollama serve & \
        OLLAMA_PID=$!; \
        sleep 5; \
        for i in 1 2 3 4 5 6 7 8 9 10; do \
            curl -s http://localhost:11434/api/tags >/dev/null 2>&1 && break; \
            sleep 2; \
        done; \
        ollama pull "${INFERENCE_MODEL}"; \
        kill $OLLAMA_PID 2>/dev/null || true; \
        echo "Model ${INFERENCE_MODEL} pre-fetched successfully"; \
    else \
        echo "Skipping model pre-fetch (PREFETCH_MODEL=0)"; \
    fi

# ---------------------------------------------------------------------------
# Compatibility with deprecated llamastack/distribution-ollama
# Usage:
#   export LLAMA_STACK_PORT=5001
#   docker run -it \
#     -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \
#     -v ~/.llama:/root/.llama \
#     cc-vec-bot \
#     --port $LLAMA_STACK_PORT \
#     --env INFERENCE_MODEL=tinyllama \
#     --env OLLAMA_URL=http://host.docker.internal:11434
#
# Or with built-in Ollama (no external Ollama needed):
#   docker run -it \
#     -p 5001:5001 -p 11434:11434 \
#     -v ~/.llama:/root/.llama \
#     cc-vec-bot \
#     --port 5001 \
#     --env INFERENCE_MODEL=tinyllama
# ---------------------------------------------------------------------------

# Default model (inherits from build ARG, can be overridden at runtime)
# tinyllama ~637MB (smallest practical LLM)
# all-minilm ~45MB (embeddings only)
# llama3.2:3b ~2GB (production)
ENV INFERENCE_MODEL=${INFERENCE_MODEL}

# Default ports
ENV LLAMA_STACK_PORT=5001
ENV OLLAMA_PORT=11434
ENV CHATBOT_PORT=8008

# ---------------------------------------------------------------------------
# Install chatbot-frontend
# ---------------------------------------------------------------------------

WORKDIR /opt/chatbot-frontend
COPY chatbot-frontend .
RUN apt-get update && apt-get install -y python3-pip && \
    pip3 install --no-cache-dir -r requirements.txt && \
    apt-get remove -y python3-pip && apt-get autoremove -y && \
    rm -rf /var/lib/apt/lists/*

# ---------------------------------------------------------------------------
# Construct entrypoint script
# ---------------------------------------------------------------------------

# Create entrypoint script compatible with distribution-ollama CLI args
RUN cat <<'EOF' > /entrypoint.sh
#!/bin/bash
set -e

# Parse --port and --env arguments (compatible with distribution-ollama)
while [[ $# -gt 0 ]]; do
    case $1 in
        --port)
            LLAMA_STACK_PORT="$2"
            shift 2
            ;;
        --env)
            # Parse KEY=VALUE and export it
            if [[ "$2" =~ ^([^=]+)=(.*)$ ]]; then
                export "${BASH_REMATCH[1]}"="${BASH_REMATCH[2]}"
            fi
            shift 2
            ;;
        *)
            # Unknown option, pass through
            EXTRA_ARGS+=("$1")
            shift
            ;;
    esac
done

echo "=============================================="
echo "cc-vec-bot (llama-stack + ollama)"
echo "=============================================="
echo "LLAMA_STACK_PORT: ${LLAMA_STACK_PORT:-5001}"
echo "INFERENCE_MODEL:  ${INFERENCE_MODEL:-tinyllama}"
echo "OLLAMA_URL:       ${OLLAMA_URL:-<local>}"
echo "=============================================="

# Determine Ollama URL
if [ -z "$OLLAMA_URL" ]; then
    # No external Ollama URL provided - start local Ollama
    echo "Starting local Ollama server..."
    ollama serve &
    OLLAMA_PID=$!
    OLLAMA_URL="http://localhost:11434"
    export OLLAMA_URL

    # Wait for Ollama to be ready
    echo "Waiting for Ollama to start..."
    for i in {1..30}; do
        if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
            echo "Ollama is ready"
            break
        fi
        if [ $i -eq 30 ]; then
            echo "ERROR: Ollama failed to start"
            exit 1
        fi
        sleep 1
    done

    # Pull model if specified
    if [ -n "$INFERENCE_MODEL" ]; then
        echo "Pulling model: $INFERENCE_MODEL"
        ollama pull "$INFERENCE_MODEL"
    fi
else
    # External Ollama URL provided - verify connectivity
    echo "Using external Ollama at: $OLLAMA_URL"
    for i in {1..10}; do
        if curl -s "${OLLAMA_URL}/api/tags" >/dev/null 2>&1; then
            echo "External Ollama is reachable"
            break
        fi
        if [ $i -eq 10 ]; then
            echo "WARNING: Cannot reach external Ollama at $OLLAMA_URL"
        fi
        sleep 1
    done
fi

# Start the chatbot in the background
echo "Starting chatbot-frontend..."
(cd /opt/chatbot-frontend && uvicorn api:app --host 0.0.0.0 --port ${CHATBOT_PORT:-8000} &)
#(cd /opt/chatbot-frontend && uvicorn api:app --host 0.0.0.0 --port ${CHATBOT_PORT:-8000})
CHATBOT_PID=$!
echo "Chatbot-frontend started with PID ${CHATBOT_PID}"

# Start llama-stack server
# The distribution-starter base image includes llama-stack
echo "Starting llama-stack on port ${LLAMA_STACK_PORT}..."
export OLLAMA_URL="${OLLAMA_URL}"
export INFERENCE_MODEL="${INFERENCE_MODEL}"
exec llama stack run starter \
    --port "${LLAMA_STACK_PORT}" \
    "${EXTRA_ARGS[@]}"
EOF

RUN chmod +x /entrypoint.sh

# Volume for llama-stack config and model cache
VOLUME ["/root/.llama"]

# Expose both llama-stack and ollama ports
EXPOSE 5001 11434

ENTRYPOINT ["/entrypoint.sh"]
CMD []
